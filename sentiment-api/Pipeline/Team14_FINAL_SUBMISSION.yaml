# PIPELINE DEFINITION
# Name: sentiment-pipeline-team14-final
# Description: Team 14 sentiment analysis with champion/challenger and evaluation
# Inputs:
#    cleaned_output_gcs: str [Default: 'gs://temp_data_mlops/cleaned_data.csv']
#    evaluation_gcs: str [Default: 'gs://temp_data_mlops/evaluation_report.json']
#    metrics_gcs: str [Default: 'gs://temp_data_mlops/train_metrics.json']
#    model_output_gcs: str [Default: 'gs://model_mlops/Team-14-v2.pickle']
#    raw_data_gcs: str [Default: 'gs://data_mlops/Movies_and_TV.csv']
#    raw_temp_gcs: str [Default: 'gs://temp_data_mlops/raw_temp.csv']
components:
  comp-clean-data:
    executorLabel: exec-clean-data
    inputDefinitions:
      parameters:
        cleaned_output_gcs:
          parameterType: STRING
        raw_temp_gcs:
          parameterType: STRING
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        raw_data_gcs:
          parameterType: STRING
        raw_temp_gcs:
          parameterType: STRING
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      parameters:
        cleaned_data_gcs:
          parameterType: STRING
        evaluation_output_gcs:
          parameterType: STRING
        metrics_input_gcs:
          parameterType: STRING
  comp-save-model-with-metadata:
    executorLabel: exec-save-model-with-metadata
    inputDefinitions:
      parameters:
        cleaned_data_gcs:
          parameterType: STRING
        metrics_input_gcs:
          parameterType: STRING
        model_output_gcs:
          parameterType: STRING
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      parameters:
        cleaned_data_gcs:
          parameterType: STRING
        metrics_output_gcs:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-clean-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - clean_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.0.3'\
          \ 'numpy==1.24.3' 'google-cloud-storage==2.10.0' 'google-api-core==2.27.0'\
          \ 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef clean_data(raw_temp_gcs: str, cleaned_output_gcs: str) -> None:\n\
          \    import pandas as pd\n    from google.cloud import storage\n\n    print(\"\
          =\"*60)\n    print(\"COMPONENT 2: CLEAN DATA\")\n    print(\"=\"*60)\n\n\
          \    client = storage.Client()\n    parts = raw_temp_gcs.replace(\"gs://\"\
          , \"\").split(\"/\")\n    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/raw.csv')\n\
          \n    df = pd.read_csv('/tmp/raw.csv', on_bad_lines='skip', engine='python')\n\
          \    print(f\"\u2713 Raw: {len(df)} rows\")\n\n    df['highrating'] = (df['rating']\
          \ >= 4).astype(int)\n    df = df[['text', 'highrating']].dropna()\n    df['text']\
          \ = df['text'].astype(str).str.lower()\n\n    print(f\"\u2713 Cleaned: {len(df)}\
          \ rows\")\n\n    df.to_csv('/tmp/cleaned.csv', index=False)\n\n    parts_out\
          \ = cleaned_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n    client.bucket(parts_out[0]).blob(\"\
          /\".join(parts_out[1:])).upload_from_filename('/tmp/cleaned.csv')\n    print(f\"\
          \u2713 Uploaded: {cleaned_output_gcs}\")\n    print(\"=\"*60)\n\n"
        image: python:3.10-slim
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage==2.10.0'\
          \ 'google-api-core==2.27.0' 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_data(raw_data_gcs: str, raw_temp_gcs: str) -> None:\n\
          \    from google.cloud import storage\n\n    print(\"=\"*60)\n    print(\"\
          COMPONENT 1: DOWNLOAD DATA\")\n    print(\"=\"*60)\n\n    client = storage.Client()\n\
          \    parts = raw_data_gcs.replace(\"gs://\", \"\").split(\"/\")\n    client.bucket(parts[0]).blob(\"\
          /\".join(parts[1:])).download_to_filename('/tmp/raw.csv')\n    print(f\"\
          \u2713 Downloaded: {raw_data_gcs}\")\n\n    parts_temp = raw_temp_gcs.replace(\"\
          gs://\", \"\").split(\"/\")\n    client.bucket(parts_temp[0]).blob(\"/\"\
          .join(parts_temp[1:])).upload_from_filename('/tmp/raw.csv')\n    print(f\"\
          \u2713 Uploaded to: {raw_temp_gcs}\")\n    print(\"=\"*60)\n\n"
        image: python:3.10-slim
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.0.3'\
          \ 'scikit-learn==1.2.2' 'numpy==1.24.3' 'scipy==1.10.1' 'google-cloud-storage==2.10.0'\
          \ 'google-api-core==2.27.0' 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(cleaned_data_gcs: str, metrics_input_gcs: str,\
          \ evaluation_output_gcs: str) -> None:\n    import pandas as pd\n    import\
          \ json\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\
          \    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection\
          \ import train_test_split\n    from sklearn.metrics import accuracy_score,\
          \ precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\
          \    from google.cloud import storage\n\n    print(\"=\"*60)\n    print(\"\
          COMPONENT 3.5: MODEL EVALUATION\")\n    print(\"=\"*60)\n\n    client =\
          \ storage.Client()\n\n    # Load cleaned data\n    parts = cleaned_data_gcs.replace(\"\
          gs://\", \"\").split(\"/\")\n    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n\
          \n    df = pd.read_csv('/tmp/cleaned.csv')\n    X, y = df['text'], df['highrating']\n\
          \n    # Same preprocessing as training\n    vectorizer = TfidfVectorizer(max_features=5000)\n\
          \    X_vec = vectorizer.fit_transform(X)\n    X_train, X_test, y_train,\
          \ y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n\n\
          \    # Train model (same as training component for consistency)\n    model\
          \ = LogisticRegression(max_iter=500, class_weight='balanced')\n    model.fit(X_train,\
          \ y_train)\n\n    # Comprehensive evaluation\n    y_pred = model.predict(X_test)\n\
          \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test,\
          \ y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test,\
          \ y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred).tolist()\n\
          \n    print(f\"\u2713 Accuracy:  {accuracy:.4f}\")\n    print(f\"\u2713\
          \ Precision: {precision:.4f}\")\n    print(f\"\u2713 Recall:    {recall:.4f}\"\
          )\n    print(f\"\u2713 F1-Score:  {f1:.4f}\")\n    print(f\"\u2713 Confusion\
          \ Matrix:\\n{conf_matrix}\")\n\n    # Detailed classification report\n \
          \   class_report = classification_report(y_test, y_pred, target_names=['Negative',\
          \ 'Positive'], output_dict=True)\n\n    # Save comprehensive evaluation\n\
          \    evaluation = {\n        'accuracy': float(accuracy),\n        'precision':\
          \ float(precision),\n        'recall': float(recall),\n        'f1_score':\
          \ float(f1),\n        'confusion_matrix': conf_matrix,\n        'classification_report':\
          \ class_report,\n        'test_samples': int(X_test.shape[0]),\n       \
          \ 'positive_samples': int(sum(y_test)),\n        'negative_samples': int(len(y_test)\
          \ - sum(y_test))\n    }\n\n    with open('/tmp/evaluation.json', 'w') as\
          \ f:\n        json.dump(evaluation, f, indent=2)\n\n    # Upload to GCS\n\
          \    parts_eval = evaluation_output_gcs.replace(\"gs://\", \"\").split(\"\
          /\")\n    client.bucket(parts_eval[0]).blob(\"/\".join(parts_eval[1:])).upload_from_filename('/tmp/evaluation.json')\n\
          \n    print(f\"\\n EVALUATION SAVED: {evaluation_output_gcs}\")\n    print(\"\
          =\"*60)\n\n"
        image: python:3.10-slim
    exec-save-model-with-metadata:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - save_model_with_metadata
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.0.3'\
          \ 'scikit-learn==1.2.2' 'numpy==1.24.3' 'scipy==1.10.1' 'google-cloud-storage==2.10.0'\
          \ 'google-api-core==2.27.0' 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef save_model_with_metadata(cleaned_data_gcs: str, model_output_gcs:\
          \ str, metrics_input_gcs: str) -> None:\n    import pickle\n    import json\n\
          \    import pandas as pd\n    from datetime import datetime\n    from sklearn.feature_extraction.text\
          \ import TfidfVectorizer\n    from sklearn.linear_model import LogisticRegression\n\
          \    from sklearn.model_selection import train_test_split\n    from sklearn.metrics\
          \ import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\
          \    from google.cloud import storage\n    import time\n\n    print(\"=\"\
          *60)\n    print(\"COMPONENT 4: SAVE MODEL WITH ENHANCED METADATA\")\n  \
          \  print(\"=\"*60)\n\n    start_time = time.time()\n    client = storage.Client()\n\
          \n    # Load cleaned data\n    parts = cleaned_data_gcs.replace(\"gs://\"\
          , \"\").split(\"/\")\n    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n\
          \n    df = pd.read_csv('/tmp/cleaned.csv')\n    X, y = df['text'], df['highrating']\n\
          \n    # Train model (same process)\n    vectorizer = TfidfVectorizer(max_features=5000)\n\
          \    X_vec = vectorizer.fit_transform(X)\n    X_train, X_test, y_train,\
          \ y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n\n\
          \    model = LogisticRegression(max_iter=500, class_weight='balanced')\n\
          \    model.fit(X_train, y_train)\n\n    # Calculate COMPREHENSIVE metrics\n\
          \    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test,\
          \ y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall =\
          \ recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    conf_matrix\
          \ = confusion_matrix(y_test, y_pred).tolist()\n\n    training_duration =\
          \ time.time() - start_time\n\n    # Check for previous version\n    parts_out\
          \ = model_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n    bucket =\
          \ client.bucket(parts_out[0])\n    model_blob_path = \"/\".join(parts_out[1:])\n\
          \    metadata_blob_path = model_blob_path.replace('.pickle', '_metadata.json')\n\
          \n    previous_version = None\n    previous_accuracy = None\n    improvement\
          \ = None\n    is_champion = True\n\n    try:\n        meta_blob = bucket.blob(metadata_blob_path)\n\
          \        old_meta = json.loads(meta_blob.download_as_text())\n        previous_version\
          \ = old_meta.get('current_version', 0)\n        previous_accuracy = old_meta.get('current_accuracy',\
          \ 0)\n\n        # Determine if new model is better\n        if accuracy\
          \ > previous_accuracy:\n            new_version = previous_version + 1\n\
          \            is_champion = True\n            improvement = ((accuracy -\
          \ previous_accuracy) / previous_accuracy) * 100\n            print(f\"\u2713\
          \ NEW CHAMPION! Accuracy improved by {improvement:.2f}%\")\n        else:\n\
          \            new_version = previous_version\n            is_champion = False\n\
          \            print(f\"\u2717 Model not better than champion. Keeping version\
          \ {previous_version}\")\n    except:\n        new_version = 1\n        print(\"\
          \u2713 First model version!\")\n\n    # Create ENHANCED metadata\n    metadata\
          \ = {\n        # Version control\n        'current_version': new_version,\n\
          \        'current_timestamp': datetime.now().isoformat(),\n        'is_champion':\
          \ is_champion,\n        'previous_version': previous_version,\n        'previous_accuracy':\
          \ previous_accuracy,\n        'improvement': improvement,\n\n        # Performance\
          \ metrics\n        'current_accuracy': float(accuracy),\n        'precision':\
          \ float(precision),\n        'recall': float(recall),\n        'f1_score':\
          \ float(f1),\n        'confusion_matrix': conf_matrix,\n        'true_negatives':\
          \ int(conf_matrix[0][0]),\n        'false_positives': int(conf_matrix[0][1]),\n\
          \        'false_negatives': int(conf_matrix[1][0]),\n        'true_positives':\
          \ int(conf_matrix[1][1]),\n\n        # Training data info\n        'samples_trained':\
          \ int(X_train.shape[0]),\n        'samples_tested': int(X_test.shape[0]),\n\
          \        'total_samples': int(len(df)),\n        'positive_samples': int(sum(y\
          \ == 1)),\n        'negative_samples': int(sum(y == 0)),\n        'class_balance':\
          \ float(sum(y == 1) / len(y)),\n        'features': int(X_vec.shape[1]),\n\
          \n        # Model configuration\n        'model_type': 'LogisticRegression',\n\
          \        'vectorizer_type': 'TfidfVectorizer',\n        'max_features':\
          \ 5000,\n        'max_iterations': 500,\n        'test_split_ratio': 0.2,\n\
          \        'random_state': 42,\n\n        # Training details\n        'training_duration_seconds':\
          \ float(training_duration),\n        'training_duration_formatted': f\"\
          {int(training_duration // 60)}m {int(training_duration % 60)}s\",\n\n  \
          \      # Dataset info\n        'dataset_source': cleaned_data_gcs,\n   \
          \     'dataset_name': 'Amazon Movies & TV Reviews',\n\n        # Team info\n\
          \        'team': 'Team-14',\n        'pipeline_name': 'sentiment-pipeline-team14-final',\n\
          \        'course': 'Data Engineering - MLOps',\n        'university': 'JADS'\n\
          \    }\n\n    # Save model\n    package = {\n        'model': model,\n \
          \       'vectorizer': vectorizer,\n        'version': new_version,\n   \
          \     'accuracy': accuracy,\n        'timestamp': datetime.now().isoformat(),\n\
          \        'samples_trained': X_train.shape[0],\n        'features': X_vec.shape[1]\n\
          \    }\n\n    with open('/tmp/model.pkl', 'wb') as f:\n        pickle.dump(package,\
          \ f)\n\n    blob_model = bucket.blob(model_blob_path)\n    blob_model.upload_from_filename('/tmp/model.pkl')\n\
          \n    # Save enhanced metadata\n    with open('/tmp/metadata.json', 'w')\
          \ as f:\n        json.dump(metadata, f, indent=2)\n\n    blob_meta = bucket.blob(metadata_blob_path)\n\
          \    blob_meta.upload_from_filename('/tmp/metadata.json')\n\n    print(f\"\
          \\n MODEL SAVED: gs://{parts_out[0]}/{model_blob_path}\")\n    print(f\"\
          \ METADATA SAVED: gs://{parts_out[0]}/{metadata_blob_path}\")\n    print(f\"\
          \\n Model Metrics:\")\n    print(f\"   Version: {new_version}\")\n    print(f\"\
          \   Accuracy: {accuracy:.4f}\")\n    print(f\"   Precision: {precision:.4f}\"\
          )\n    print(f\"   Recall: {recall:.4f}\")\n    print(f\"   F1-Score: {f1:.4f}\"\
          )\n    print(f\"   Training Duration: {training_duration:.2f}s\")\n    print(\"\
          =\"*60)\n\n"
        image: python:3.10-slim
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'pandas==2.0.3'\
          \ 'scikit-learn==1.2.2' 'numpy==1.24.3' 'scipy==1.10.1' 'google-cloud-storage==2.10.0'\
          \ 'google-api-core==2.27.0' 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(cleaned_data_gcs: str, metrics_output_gcs: str) ->\
          \ None:\n    import pandas as pd\n    import json\n    from sklearn.feature_extraction.text\
          \ import TfidfVectorizer\n    from sklearn.linear_model import LogisticRegression\n\
          \    from sklearn.model_selection import train_test_split\n    from sklearn.metrics\
          \ import accuracy_score\n    from google.cloud import storage\n\n    print(\"\
          =\"*60)\n    print(\"COMPONENT 3: TRAIN MODEL\")\n    print(\"=\"*60)\n\n\
          \    client = storage.Client()\n    parts = cleaned_data_gcs.replace(\"\
          gs://\", \"\").split(\"/\")\n    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n\
          \n    df = pd.read_csv('/tmp/cleaned.csv')\n    X, y = df['text'], df['highrating']\n\
          \n    vectorizer = TfidfVectorizer(max_features=5000)\n    X_vec = vectorizer.fit_transform(X)\n\
          \    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2,\
          \ random_state=42)\n\n    model = LogisticRegression(max_iter=500, class_weight='balanced')\n\
          \    model.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test,\
          \ model.predict(X_test))\n\n    print(f\"\u2713 Accuracy: {accuracy:.4f}\"\
          )\n    print(f\"\u2713 Training samples: {X_train.shape[0]}\")  # \u2190\
          \ FIXED\n    print(\"=\"*60)\n\n    # Write metrics to GCS\n    metrics\
          \ = {\n        'accuracy': float(accuracy),\n        'samples': int(X_train.shape[0]),\
          \  # \u2190 FIXED\n        'features': int(X_vec.shape[1])\n    }\n\n  \
          \  with open('/tmp/metrics.json', 'w') as f:\n        json.dump(metrics,\
          \ f)\n\n    parts_metrics = metrics_output_gcs.replace(\"gs://\", \"\").split(\"\
          /\")\n    client.bucket(parts_metrics[0]).blob(\"/\".join(parts_metrics[1:])).upload_from_filename('/tmp/metrics.json')\n\
          \    print(f\"\u2713 Metrics saved to: {metrics_output_gcs}\")\n\n"
        image: python:3.10-slim
pipelineInfo:
  description: Team 14 sentiment analysis with champion/challenger and evaluation
  name: sentiment-pipeline-team14-final
root:
  dag:
    tasks:
      clean-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-clean-data
        dependentTasks:
        - download-data
        inputs:
          parameters:
            cleaned_output_gcs:
              componentInputParameter: cleaned_output_gcs
            raw_temp_gcs:
              componentInputParameter: raw_temp_gcs
        taskInfo:
          name: clean-data
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            raw_data_gcs:
              componentInputParameter: raw_data_gcs
            raw_temp_gcs:
              componentInputParameter: raw_temp_gcs
        taskInfo:
          name: download-data
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - train-model
        inputs:
          parameters:
            cleaned_data_gcs:
              componentInputParameter: cleaned_output_gcs
            evaluation_output_gcs:
              componentInputParameter: evaluation_gcs
            metrics_input_gcs:
              componentInputParameter: metrics_gcs
        taskInfo:
          name: evaluate-model
      save-model-with-metadata:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-save-model-with-metadata
        dependentTasks:
        - evaluate-model
        inputs:
          parameters:
            cleaned_data_gcs:
              componentInputParameter: cleaned_output_gcs
            metrics_input_gcs:
              componentInputParameter: metrics_gcs
            model_output_gcs:
              componentInputParameter: model_output_gcs
        taskInfo:
          name: save-model-with-metadata
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - clean-data
        inputs:
          parameters:
            cleaned_data_gcs:
              componentInputParameter: cleaned_output_gcs
            metrics_output_gcs:
              componentInputParameter: metrics_gcs
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      cleaned_output_gcs:
        defaultValue: gs://temp_data_mlops/cleaned_data.csv
        isOptional: true
        parameterType: STRING
      evaluation_gcs:
        defaultValue: gs://temp_data_mlops/evaluation_report.json
        isOptional: true
        parameterType: STRING
      metrics_gcs:
        defaultValue: gs://temp_data_mlops/train_metrics.json
        isOptional: true
        parameterType: STRING
      model_output_gcs:
        defaultValue: gs://model_mlops/Team-14-v2.pickle
        isOptional: true
        parameterType: STRING
      raw_data_gcs:
        defaultValue: gs://data_mlops/Movies_and_TV.csv
        isOptional: true
        parameterType: STRING
      raw_temp_gcs:
        defaultValue: gs://temp_data_mlops/raw_temp.csv
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0
