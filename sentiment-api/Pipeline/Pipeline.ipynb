{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ca534ca-908c-433a-beb0-30b969a3a720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.75.1 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Install\n",
    "!pip uninstall protobuf -y --quiet\n",
    "!pip install protobuf==3.20.3 --quiet\n",
    "!pip install kfp==2.3.0 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ceffb80-7532-4ccd-b083-f63300fe75e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "from kfp.dsl import component, pipeline\n",
    "from kfp import compiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df088b59-8055-464b-81db-0e6a2397fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Component 1: Download\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage==2.10.0\", \"google-api-core==2.27.0\"],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def download_data(raw_data_gcs: str, raw_temp_gcs: str) -> None:\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPONENT 1: DOWNLOAD DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    parts = raw_data_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/raw.csv')\n",
    "    print(f\"✓ Downloaded: {raw_data_gcs}\")\n",
    "    \n",
    "    parts_temp = raw_temp_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts_temp[0]).blob(\"/\".join(parts_temp[1:])).upload_from_filename('/tmp/raw.csv')\n",
    "    print(f\"✓ Uploaded to: {raw_temp_gcs}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcbd3bc8-ad08-455f-b4f7-b1ac1d2ac070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4 - Component 2: Clean\n",
    "@component(\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\", \"google-cloud-storage==2.10.0\", \"google-api-core==2.27.0\"],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def clean_data(raw_temp_gcs: str, cleaned_output_gcs: str) -> None:\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPONENT 2: CLEAN DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    parts = raw_temp_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/raw.csv')\n",
    "    \n",
    "    df = pd.read_csv('/tmp/raw.csv', on_bad_lines='skip', engine='python')\n",
    "    print(f\"✓ Raw: {len(df)} rows\")\n",
    "    \n",
    "    df['highrating'] = (df['rating'] >= 4).astype(int)\n",
    "    df = df[['text', 'highrating']].dropna()\n",
    "    df['text'] = df['text'].astype(str).str.lower()\n",
    "    \n",
    "    print(f\"✓ Cleaned: {len(df)} rows\")\n",
    "    \n",
    "    df.to_csv('/tmp/cleaned.csv', index=False)\n",
    "    \n",
    "    parts_out = cleaned_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts_out[0]).blob(\"/\".join(parts_out[1:])).upload_from_filename('/tmp/cleaned.csv')\n",
    "    print(f\"✓ Uploaded: {cleaned_output_gcs}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df5fb42e-2be7-4c02-be4b-51ee19fd1775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5 - Component 3: Train Model \n",
    "@component(\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.2.2\", \"numpy==1.24.3\", \"scipy==1.10.1\", \"google-cloud-storage==2.10.0\", \"google-api-core==2.27.0\"],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def train_model(cleaned_data_gcs: str, metrics_output_gcs: str) -> None:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPONENT 3: TRAIN MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    parts = cleaned_data_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n",
    "    \n",
    "    df = pd.read_csv('/tmp/cleaned.csv')\n",
    "    X, y = df['text'], df['highrating']\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"✓ Training samples: {X_train.shape[0]}\")  # ← FIXED\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Write metrics to GCS\n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'samples': int(X_train.shape[0]),  # ← FIXED\n",
    "        'features': int(X_vec.shape[1])\n",
    "    }\n",
    "    \n",
    "    with open('/tmp/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    parts_metrics = metrics_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts_metrics[0]).blob(\"/\".join(parts_metrics[1:])).upload_from_filename('/tmp/metrics.json')\n",
    "    print(f\"✓ Metrics saved to: {metrics_output_gcs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d93b407-6ee8-4bb4-8304-fbaa6522543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.5 - Component 3.5: Evaluate Model\n",
    "@component(\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.2.2\", \"numpy==1.24.3\", \"scipy==1.10.1\", \"google-cloud-storage==2.10.0\", \"google-api-core==2.27.0\"],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def evaluate_model(cleaned_data_gcs: str, metrics_input_gcs: str, evaluation_output_gcs: str) -> None:\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPONENT 3.5: MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Load cleaned data\n",
    "    parts = cleaned_data_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n",
    "    \n",
    "    df = pd.read_csv('/tmp/cleaned.csv')\n",
    "    X, y = df['text'], df['highrating']\n",
    "    \n",
    "    # Same preprocessing as training\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model (same as training component for consistency)\n",
    "    model = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred).tolist()\n",
    "    \n",
    "    print(f\"✓ Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"✓ Precision: {precision:.4f}\")\n",
    "    print(f\"✓ Recall:    {recall:.4f}\")\n",
    "    print(f\"✓ F1-Score:  {f1:.4f}\")\n",
    "    print(f\"✓ Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'], output_dict=True)\n",
    "    \n",
    "    # Save comprehensive evaluation\n",
    "    evaluation = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report,\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'positive_samples': int(sum(y_test)),\n",
    "        'negative_samples': int(len(y_test) - sum(y_test))\n",
    "    }\n",
    "    \n",
    "    with open('/tmp/evaluation.json', 'w') as f:\n",
    "        json.dump(evaluation, f, indent=2)\n",
    "    \n",
    "    # Upload to GCS\n",
    "    parts_eval = evaluation_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts_eval[0]).blob(\"/\".join(parts_eval[1:])).upload_from_filename('/tmp/evaluation.json')\n",
    "    \n",
    "    print(f\"\\n EVALUATION SAVED: {evaluation_output_gcs}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e609625f-ca0b-4701-9432-2cb9bc2ba3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6 - Component 4: Save Model with Metadata\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.2.2\",\n",
    "        \"numpy==1.24.3\",\n",
    "        \"scipy==1.10.1\",\n",
    "        \"google-cloud-storage==2.10.0\",\n",
    "        \"google-api-core==2.27.0\"\n",
    "    ],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def save_model_with_metadata(cleaned_data_gcs: str, model_output_gcs: str, metrics_input_gcs: str) -> None:\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    from google.cloud import storage\n",
    "    import time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPONENT 4: SAVE MODEL WITH ENHANCED METADATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Load cleaned data\n",
    "    parts = cleaned_data_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    client.bucket(parts[0]).blob(\"/\".join(parts[1:])).download_to_filename('/tmp/cleaned.csv')\n",
    "    \n",
    "    df = pd.read_csv('/tmp/cleaned.csv')\n",
    "    X, y = df['text'], df['highrating']\n",
    "    \n",
    "    # Train model (same process)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate COMPREHENSIVE metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred).tolist()\n",
    "    \n",
    "    training_duration = time.time() - start_time\n",
    "    \n",
    "    # Check for previous version\n",
    "    parts_out = model_output_gcs.replace(\"gs://\", \"\").split(\"/\")\n",
    "    bucket = client.bucket(parts_out[0])\n",
    "    model_blob_path = \"/\".join(parts_out[1:])\n",
    "    metadata_blob_path = model_blob_path.replace('.pickle', '_metadata.json')\n",
    "    \n",
    "    previous_version = None\n",
    "    previous_accuracy = None\n",
    "    improvement = None\n",
    "    is_champion = True\n",
    "    \n",
    "    try:\n",
    "        meta_blob = bucket.blob(metadata_blob_path)\n",
    "        old_meta = json.loads(meta_blob.download_as_text())\n",
    "        previous_version = old_meta.get('current_version', 0)\n",
    "        previous_accuracy = old_meta.get('current_accuracy', 0)\n",
    "        \n",
    "        # Determine if new model is better\n",
    "        if accuracy > previous_accuracy:\n",
    "            new_version = previous_version + 1\n",
    "            is_champion = True\n",
    "            improvement = ((accuracy - previous_accuracy) / previous_accuracy) * 100\n",
    "            print(f\"✓ NEW CHAMPION! Accuracy improved by {improvement:.2f}%\")\n",
    "        else:\n",
    "            new_version = previous_version\n",
    "            is_champion = False\n",
    "            print(f\"✗ Model not better than champion. Keeping version {previous_version}\")\n",
    "    except:\n",
    "        new_version = 1\n",
    "        print(\"✓ First model version!\")\n",
    "    \n",
    "    # Create ENHANCED metadata\n",
    "    metadata = {\n",
    "        # Version control\n",
    "        'current_version': new_version,\n",
    "        'current_timestamp': datetime.now().isoformat(),\n",
    "        'is_champion': is_champion,\n",
    "        'previous_version': previous_version,\n",
    "        'previous_accuracy': previous_accuracy,\n",
    "        'improvement': improvement,\n",
    "        \n",
    "        # Performance metrics\n",
    "        'current_accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'true_negatives': int(conf_matrix[0][0]),\n",
    "        'false_positives': int(conf_matrix[0][1]),\n",
    "        'false_negatives': int(conf_matrix[1][0]),\n",
    "        'true_positives': int(conf_matrix[1][1]),\n",
    "        \n",
    "        # Training data info\n",
    "        'samples_trained': int(X_train.shape[0]),\n",
    "        'samples_tested': int(X_test.shape[0]),\n",
    "        'total_samples': int(len(df)),\n",
    "        'positive_samples': int(sum(y == 1)),\n",
    "        'negative_samples': int(sum(y == 0)),\n",
    "        'class_balance': float(sum(y == 1) / len(y)),\n",
    "        'features': int(X_vec.shape[1]),\n",
    "        \n",
    "        # Model configuration\n",
    "        'model_type': 'LogisticRegression',\n",
    "        'vectorizer_type': 'TfidfVectorizer',\n",
    "        'max_features': 5000,\n",
    "        'max_iterations': 500,\n",
    "        'test_split_ratio': 0.2,\n",
    "        'random_state': 42,\n",
    "        \n",
    "        # Training details\n",
    "        'training_duration_seconds': float(training_duration),\n",
    "        'training_duration_formatted': f\"{int(training_duration // 60)}m {int(training_duration % 60)}s\",\n",
    "        \n",
    "        # Dataset info\n",
    "        'dataset_source': cleaned_data_gcs,\n",
    "        'dataset_name': 'Amazon Movies & TV Reviews',\n",
    "        \n",
    "        # Team info\n",
    "        'team': 'Team-14',\n",
    "        'pipeline_name': 'sentiment-pipeline-team14-final',\n",
    "        'course': 'Data Engineering - MLOps',\n",
    "        'university': 'JADS'\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    package = {\n",
    "        'model': model,\n",
    "        'vectorizer': vectorizer,\n",
    "        'version': new_version,\n",
    "        'accuracy': accuracy,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'samples_trained': X_train.shape[0],\n",
    "        'features': X_vec.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open('/tmp/model.pkl', 'wb') as f:\n",
    "        pickle.dump(package, f)\n",
    "    \n",
    "    blob_model = bucket.blob(model_blob_path)\n",
    "    blob_model.upload_from_filename('/tmp/model.pkl')\n",
    "    \n",
    "    # Save enhanced metadata\n",
    "    with open('/tmp/metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    blob_meta = bucket.blob(metadata_blob_path)\n",
    "    blob_meta.upload_from_filename('/tmp/metadata.json')\n",
    "    \n",
    "    print(f\"\\n MODEL SAVED: gs://{parts_out[0]}/{model_blob_path}\")\n",
    "    print(f\" METADATA SAVED: gs://{parts_out[0]}/{metadata_blob_path}\")\n",
    "    print(f\"\\n Model Metrics:\")\n",
    "    print(f\"   Version: {new_version}\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"   Training Duration: {training_duration:.2f}s\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2c5efb6-2b5f-4023-a8f6-40721a51fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Pipeline Definition (UPDATED with evaluation)\n",
    "@pipeline(\n",
    "    name='sentiment-pipeline-team14-final',\n",
    "    description='Team 14 sentiment analysis with champion/challenger and evaluation'\n",
    ")\n",
    "def sentiment_pipeline(\n",
    "    raw_data_gcs: str = 'gs://data_mlops/Movies_and_TV.csv',\n",
    "    raw_temp_gcs: str = 'gs://temp_data_mlops/raw_temp.csv',\n",
    "    cleaned_output_gcs: str = 'gs://temp_data_mlops/cleaned_data.csv',\n",
    "    model_output_gcs: str = 'gs://model_mlops/Team-14-v2.pickle',\n",
    "    metrics_gcs: str = 'gs://temp_data_mlops/train_metrics.json',\n",
    "    evaluation_gcs: str = 'gs://temp_data_mlops/evaluation_report.json'\n",
    "):\n",
    "    # Component 1: Download\n",
    "    download_task = download_data(raw_data_gcs=raw_data_gcs, raw_temp_gcs=raw_temp_gcs)\n",
    "    \n",
    "    # Component 2: Clean\n",
    "    clean_task = clean_data(raw_temp_gcs=raw_temp_gcs, cleaned_output_gcs=cleaned_output_gcs)\n",
    "    clean_task.after(download_task)\n",
    "    \n",
    "    # Component 3: Train\n",
    "    train_task = train_model(cleaned_data_gcs=cleaned_output_gcs, metrics_output_gcs=metrics_gcs)\n",
    "    train_task.after(clean_task)\n",
    "    \n",
    "    # Component 3.5: Evaluate (NEW!)\n",
    "    eval_task = evaluate_model(\n",
    "        cleaned_data_gcs=cleaned_output_gcs,\n",
    "        metrics_input_gcs=metrics_gcs,\n",
    "        evaluation_output_gcs=evaluation_gcs\n",
    "    )\n",
    "    eval_task.after(train_task)\n",
    "    \n",
    "    # Component 4: Save Model\n",
    "    save_task = save_model_with_metadata(\n",
    "        cleaned_data_gcs=cleaned_output_gcs,\n",
    "        model_output_gcs=model_output_gcs,\n",
    "        metrics_input_gcs=metrics_gcs\n",
    "    )\n",
    "    save_task.after(eval_task)  # Changed from train_task to eval_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "986774ef-a9c5-4bd9-92fb-f93296d03b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Compiled: Team14_FINAL_SUBMISSION.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 - Compile\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=sentiment_pipeline,\n",
    "    package_path='Team14_FINAL_SUBMISSION.yaml'\n",
    ")\n",
    "print(\" Compiled: Team14_FINAL_SUBMISSION.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec586b-2ddb-4829-9084-ba24bbad2404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
