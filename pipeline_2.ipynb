{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed2eab8-af80-445a-8e25-74d81d2df6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.75.1 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fix protobuf conflict and install KFP\n",
    "!pip uninstall protobuf -y --quiet\n",
    "!pip install protobuf==3.20.3 --quiet\n",
    "!pip install kfp==2.3.0 --quiet\n",
    "\n",
    "print(\" Packages installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade226a0-dd9a-4232-a706-b35291999ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from kfp.dsl import component, pipeline\n",
    "from kfp import compiler\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2ab5ab-1af2-4ff8-995f-e5123d25ea4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-storage==2.10.0\", \n",
    "        \"pandas==2.0.3\", \n",
    "        \"scikit-learn==1.2.2\",\n",
    "        \"numpy==1.24.3\"\n",
    "    ],\n",
    "    base_image=\"python:3.10-slim\"\n",
    ")\n",
    "def train_sentiment_model(\n",
    "    raw_data_gcs: str,\n",
    "    model_output_gcs: str\n",
    ") -> NamedTuple('Outputs', [('accuracy', float), ('deployed', bool)]):\n",
    "    \"\"\"\n",
    "    Train sentiment analysis model with champion/challenger comparison.\n",
    "    \n",
    "    Args:\n",
    "        raw_data_gcs: GCS path to raw CSV data (gs://bucket/file.csv)\n",
    "        model_output_gcs: GCS path where model should be saved\n",
    "        \n",
    "    Returns:\n",
    "        accuracy: Accuracy score of new model\n",
    "        deployed: Whether new model was deployed (True/False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import libraries (inside component)\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import re\n",
    "    from google.cloud import storage\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from collections import namedtuple\n",
    "    \n",
    "\n",
    "    # STEP 1: Download Data from GCS\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: Downloading data from GCS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket_name = raw_data_gcs.replace(\"gs://\", \"\").split(\"/\")[0]\n",
    "    blob_path = \"/\".join(raw_data_gcs.replace(\"gs://\", \"\").split(\"/\")[1:])\n",
    "    \n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.download_to_filename(\"/tmp/raw_data.csv\")\n",
    "    \n",
    "    print(f\"✓ Downloaded from: {raw_data_gcs}\")\n",
    "    \n",
    "\n",
    "    # STEP 2: Load and Validate Data\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Loading and validating data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load with error handling for corrupted rows\n",
    "    try:\n",
    "        df = pd.read_csv(\"/tmp/raw_data.csv\", on_bad_lines='skip', nrows=2000)\n",
    "    except:\n",
    "        df = pd.read_csv(\"/tmp/raw_data.csv\", \n",
    "                        error_bad_lines=False, \n",
    "                        warn_bad_lines=False, \n",
    "                        nrows=2000)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(df)} rows\")\n",
    "    print(f\"✓ Columns: {list(df.columns)}\")\n",
    "    \n",
    "\n",
    "    # STEP 3: Data Preprocessing\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Preprocessing data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clean text: lowercase, remove special chars\n",
    "    df['clean_text'] = df['text'].astype(str).str.lower()\n",
    "    df['clean_text'] = df['clean_text'].str.replace('[^a-z ]', '', regex=True)\n",
    "    \n",
    "    # Filter out very short text\n",
    "    df = df[df['clean_text'].str.len() > 10]\n",
    "    \n",
    "    # Create binary labels (4+ stars = positive)\n",
    "    df['label'] = (df['rating'] >= 4).astype(int)\n",
    "    \n",
    "    print(f\"✓ Cleaned text\")\n",
    "    print(f\"✓ Final dataset: {len(df)} rows\")\n",
    "    print(f\"✓ Positive samples: {df['label'].sum()}\")\n",
    "    print(f\"✓ Negative samples: {len(df) - df['label'].sum()}\")\n",
    "    \n",
    "\n",
    "    # STEP 4: Feature Engineering\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: Creating features\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create bag-of-words features\n",
    "    vectorizer = CountVectorizer(max_features=50, min_df=2)\n",
    "    X = vectorizer.fit_transform(df['clean_text'])\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"✓ Feature matrix shape: {X.shape}\")\n",
    "    print(f\"✓ Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train samples: {X_train.shape[0]}\")\n",
    "    print(f\"✓ Test samples: {X_test.shape[0]}\")\n",
    "    \n",
    "\n",
    "    # STEP 5: Train New Model\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: Training new model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    new_model = MultinomialNB()\n",
    "    new_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = new_model.predict(X_test)\n",
    "    new_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"✓ Model trained successfully\")\n",
    "    print(f\"✓ New model accuracy: {new_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "    # STEP 6: Load Old Model (if exists)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 6: Checking for existing production model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    old_model_exists = False\n",
    "    old_accuracy = 0.0\n",
    "    \n",
    "    try:\n",
    "        # Try to load existing model from GCS\n",
    "        model_bucket_name = model_output_gcs.replace(\"gs://\", \"\").split(\"/\")[0]\n",
    "        model_blob_path = \"/\".join(model_output_gcs.replace(\"gs://\", \"\").split(\"/\")[1:])\n",
    "        \n",
    "        model_bucket = client.bucket(model_bucket_name)\n",
    "        model_blob = model_bucket.blob(model_blob_path)\n",
    "        \n",
    "        if model_blob.exists():\n",
    "            print(\"✓ Found existing model in production\")\n",
    "            model_blob.download_to_filename(\"/tmp/old_model.pkl\")\n",
    "            \n",
    "            with open(\"/tmp/old_model.pkl\", \"rb\") as f:\n",
    "                old_package = pickle.load(f)\n",
    "                old_accuracy = old_package.get('accuracy', 0.0)\n",
    "            \n",
    "            print(f\"✓ Production model accuracy: {old_accuracy:.4f}\")\n",
    "            old_model_exists = True\n",
    "        else:\n",
    "            print(\"✓ No existing model found (first deployment)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✓ No existing model found: {str(e)[:50]}\")\n",
    "    \n",
    "\n",
    "    # STEP 7: Model Comparison & Deployment Decision\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 7: Model comparison and deployment decision\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    should_deploy = False\n",
    "    reason = \"\"\n",
    "    \n",
    "    if not old_model_exists:\n",
    "        should_deploy = True\n",
    "        reason = \"No existing model - deploying baseline\"\n",
    "    elif new_accuracy > old_accuracy:\n",
    "        should_deploy = True\n",
    "        improvement = (new_accuracy - old_accuracy) * 100\n",
    "        reason = f\"New model is better (+{improvement:.2f}% improvement)\"\n",
    "    else:\n",
    "        should_deploy = False\n",
    "        decline = (old_accuracy - new_accuracy) * 100\n",
    "        reason = f\"Old model is better (-{decline:.2f}% decline)\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPARISON RESULTS:\")\n",
    "    print(f\"  New Model Accuracy: {new_accuracy:.4f}\")\n",
    "    print(f\"  Old Model Accuracy: {old_accuracy:.4f}\")\n",
    "    print(f\"\\nDECISION: {'✓ DEPLOY NEW MODEL' if should_deploy else '✗ KEEP OLD MODEL'}\")\n",
    "    print(f\"REASON: {reason}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "\n",
    "    # STEP 8: Save Model (if deployment approved)\n",
    "\n",
    "    if should_deploy:\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 8: Deploying new model to production\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Package model with metadata\n",
    "        model_package = {\n",
    "            'model': new_model,\n",
    "            'vectorizer': vectorizer,\n",
    "            'accuracy': float(new_accuracy),\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'samples_trained': int(X_train.shape[0]),\n",
    "            'features': int(X.shape[1])\n",
    "        }\n",
    "        \n",
    "        # Save locally\n",
    "        with open(\"/tmp/model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        # Upload to GCS\n",
    "        model_bucket = client.bucket(model_bucket_name)\n",
    "        model_blob = model_bucket.blob(model_blob_path)\n",
    "        model_blob.upload_from_filename(\"/tmp/model.pkl\")\n",
    "        \n",
    "        print(f\"✓ Model deployed to: {model_output_gcs}\")\n",
    "        print(f\"✓ Model accuracy: {new_accuracy:.4f}\")\n",
    "        print(f\"✓ Training samples: {X_train.shape[0]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 8: Keeping existing production model\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"✓ Production model unchanged\")\n",
    "        print(f\"✓ Current accuracy: {old_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "    # PIPELINE COMPLETION\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" PIPELINE EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Return outputs\n",
    "    output = namedtuple('Outputs', ['accuracy', 'deployed'])\n",
    "    return output(new_accuracy, should_deploy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edb2ec9-4818-4fb2-a6e3-335f1073cdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name='sentiment-analysis-training-pipeline',\n",
    "    description='Continuous training pipeline for sentiment analysis with model comparison'\n",
    ")\n",
    "def sentiment_training_pipeline(\n",
    "    raw_data_gcs: str = 'gs://data_mlops/Movies_and_TV.csv',\n",
    "    model_output_gcs: str = 'gs://model_mlops/sentiment_model.pickle'\n",
    "):\n",
    "    \"\"\"\n",
    "    Main pipeline definition.\n",
    "    \n",
    "    Args:\n",
    "        raw_data_gcs: Path to raw data in GCS\n",
    "        model_output_gcs: Path where trained model should be stored\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute training component\n",
    "    train_task = train_sentiment_model(\n",
    "        raw_data_gcs=raw_data_gcs,\n",
    "        model_output_gcs=model_output_gcs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f924da69-c2ae-459e-afa0-1048da91f8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline compiled successfully!\n",
      " Output file: sentiment_training_pipeline.json\n",
      "\n",
      " Next steps:\n",
      "   1. Download the JSON file\n",
      "   2. Go to Vertex AI > Pipelines\n",
      "   3. Click 'CREATE RUN'\n",
      "   4. Upload sentiment_training_pipeline.json\n",
      "   5. Fill in parameters and submit\n"
     ]
    }
   ],
   "source": [
    "# Compile pipeline to JSON\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=sentiment_training_pipeline,\n",
    "    package_path='sentiment_training_pipeline.json'\n",
    ")\n",
    "\n",
    "print(\" Pipeline compiled successfully!\")\n",
    "print(\" Output file: sentiment_training_pipeline.json\")\n",
    "print(\"\\n Next steps:\")\n",
    "print(\"   1. Download the JSON file\")\n",
    "print(\"   2. Go to Vertex AI > Pipelines\")\n",
    "print(\"   3. Click 'CREATE RUN'\")\n",
    "print(\"   4. Upload sentiment_training_pipeline.json\")\n",
    "print(\"   5. Fill in parameters and submit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947922da-7798-4333-aef2-cfc353dc7475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration ready for Vertex AI submission\n"
     ]
    }
   ],
   "source": [
    "# Pipeline parameters for Vertex AI submission\n",
    "PIPELINE_PARAMETERS = {\n",
    "    'raw_data_gcs': 'gs://data_mlops/Movies_and_TV.csv',\n",
    "    'model_output_gcs': 'gs://model_mlops/sentiment_model.pickle'\n",
    "}\n",
    "\n",
    "# Runtime configuration\n",
    "RUNTIME_CONFIG = {\n",
    "    'output_directory': 'gs://temp_data_mlops/',\n",
    "    'failure_policy': 'Run all steps to completion',\n",
    "    'cache_configuration': 'Do not override task-level cache configuration'\n",
    "}\n",
    "\n",
    "print(\" Configuration ready for Vertex AI submission\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05585c7-57d0-4c81-ba28-bdc468d8bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
